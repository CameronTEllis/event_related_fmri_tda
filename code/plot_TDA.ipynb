{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate plots for event-related fMRI with TDA\n",
    "This notebook contains the tools for displaying the output of the event-related fMRI data analyses found at: https://github.com/CameronTEllis/event_related_fmri_tda. \n",
    "\n",
    "This assumes 1) you ran at least some of the simulation/analyses of the data (using \"./code/supervisor_supersubject.sh\" for instance). More explanation for this will be provided [below](#supersubject_script). 2) you are able to launch the jupyter notebook with the same environment that you used to run the code.\n",
    "\n",
    "Contents:\n",
    "\n",
    "\n",
    "[Setup](#setup)  \n",
    "\n",
    "[Check that the appropriate analyses have been run](#check)  \n",
    "\n",
    ">[Print default script for running all of the main analyses](#supersubject_script)  \n",
    "\n",
    "[Validate the simulation accuracy](#validate)  \n",
    "\n",
    "[Pull out the MDS representations from the fully simulated data](#mds)  \n",
    "\n",
    "[Make max loop signal plots](#max_loop)  \n",
    "\n",
    "[Make the plots for the matching loop number](#proportion_loop)  \n",
    "\n",
    "[How many searchlights contain the ROI](#searchlights_in_ROI)  \n",
    "\n",
    "[Generate the elements for the methods figure in the manuscript](#figure_1)  \n",
    ">[Plot a brain slice](#figure_1_slice)  \n",
    ">[Plot a distance matrix](#figure_1_dist)  \n",
    ">[Run persistent homology on a searchlight voxel and plot it](#figure_1_PH)  \n",
    "\n",
    "[Plot example signal activity and signal plus noise](#figure_S4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a> \n",
    "## Setup some functions that will be used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import nibabel as nb\n",
    "import scipy.spatial.distance as sp_distance\n",
    "import sklearn.manifold as manifold\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "import generate_graph_structure as graphs\n",
    "import test_graph_structure as test_graphs\n",
    "import glob\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_node_brain_dist(filename, mask_vec):\n",
    "    nii = nb.load(filename)\n",
    "    data_vol = nii.get_data()\n",
    "\n",
    "    # Turn the data into a vector\n",
    "    data_vec = data_vol.reshape((np.prod(data_vol.shape[0:3])), data_vol.shape[3])\n",
    "\n",
    "    # Mask the data\n",
    "    data_masked = data_vec[mask_vec]\n",
    "\n",
    "    # Average the voxels in the masked region\n",
    "    data_av = np.mean(data_masked, 0)\n",
    "\n",
    "    # Reshape the data to be node by node\n",
    "    nodes = int(np.ceil(np.sqrt(len(data_av) * 2)))\n",
    "\n",
    "    # What are the indices for the upper triangle\n",
    "    idxs_u = np.triu_indices(nodes, 1)\n",
    "    idxs_l = np.tril_indices(nodes, -1)\n",
    "\n",
    "    # Insert the data into a dist matrix\n",
    "    data_dist = np.zeros((nodes, nodes))\n",
    "\n",
    "    # Add the data\n",
    "    data_dist[idxs_u[0], idxs_u[1]] = data_av\n",
    "\n",
    "    # Symmetrize the data\n",
    "    data_dist = (data_dist.T + data_dist) / 2\n",
    "    \n",
    "    # Return the distance matrix\n",
    "    return data_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a plot of the summary for a given condition\n",
    "def plot_summary_stats(data, keys, line_style, color):\n",
    "    \n",
    "    plotting_data = []\n",
    "    scatter_data = []\n",
    "    plotting_error = []\n",
    "    ppt_num = 20 # What is the expected number of ppts\n",
    "    for key in keys:\n",
    "        \n",
    "        # Pull out the data\n",
    "        data_point = data[key]\n",
    "        \n",
    "        # Is this a dictionary\n",
    "        if isinstance(data_point, dict):\n",
    "            # If these are different participants then average them and add error bars\n",
    "            ppt_data = list(data_point.values())\n",
    "            \n",
    "            # Do something different for the max signal points\n",
    "            if key.find('s-5.0') < 0:\n",
    "                plotting_data += [np.nanmean(ppt_data)]\n",
    "                plotting_error += [np.nanstd(ppt_data) / np.sqrt(len(ppt_data))]\n",
    "            else:\n",
    "                scatter_data += [5, np.nanmean(ppt_data)]\n",
    "            \n",
    "            if len(ppt_data) != ppt_num:\n",
    "                print('%s has only %d ppts' % (key, len(ppt_data)))\n",
    "            \n",
    "        else:\n",
    "            # Do something different for the max signal points\n",
    "            if key.find('s-5.0') < 0:\n",
    "                plotting_data += [data_point]\n",
    "            else:\n",
    "                scatter_data += [5, data_point]\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.plot(plotting_data, line_style, color= color, linewidth=5)\n",
    "    if len(plotting_error) == len(plotting_data):\n",
    "        plt.errorbar(np.arange(len(plotting_data)), plotting_data, plotting_error, linestyle=line_style, ecolor=color, color=color, linewidth=5)\n",
    "    \n",
    "    # Scatter the data\n",
    "    if len(scatter_data) > 0:\n",
    "        if line_style == '-':\n",
    "            plt.plot(scatter_data[0], scatter_data[1], markeredgecolor=color, marker='o', fillstyle='none', markersize= 20, markeredgewidth=3)\n",
    "    #        plt.scatter(scatter_data[0], scatter_data[1], color=color, hatch='/' * 5, alpha=0.4, s= 250)\n",
    "        else:\n",
    "            plt.scatter(scatter_data[0], scatter_data[1], color=color, alpha=0.5, s= 250)     \n",
    "     \n",
    "    # Return the data\n",
    "    return plotting_data, scatter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compare the different inputs\n",
    "def plot_boxplot(*arg,\n",
    "                 ):\n",
    "    plt.figure(figsize=(4, len(arg) * 3))\n",
    "    \n",
    "    # Reshape and store the data\n",
    "\n",
    "    for arg_number in list(range(0, len(arg))):\n",
    "        var = arg[arg_number]\n",
    "        var = var.reshape(np.prod(var.shape))\n",
    "\n",
    "        # Add the data to the matrix\n",
    "        if arg_number == 0:\n",
    "            data = np.asarray([var])\n",
    "        else:\n",
    "            data = np.vstack([data, var])\n",
    "\n",
    "    # What is the x value\n",
    "    x_idxs = list(range(1, data.shape[0] + 1))\n",
    "    \n",
    "    plt.plot(x_idxs, data, c=(0.9,0.9,0.9))\n",
    "    \n",
    "    # What indexes are usable from the data\n",
    "    useable_idxs = np.where(np.prod(~np.isnan(data), axis=0)==1)[0]\n",
    "    \n",
    "    # Plot the box plots\n",
    "    plt.boxplot(np.transpose(data[:, useable_idxs]))\n",
    "    \n",
    "    # If there are two inputs, do a t test\n",
    "    if len(arg) == 2:\n",
    "        t, p = stats.ttest_rel(data[1, useable_idxs], data[0, useable_idxs])\n",
    "        delta_mean = np.nanmean(np.subtract(data[1, useable_idxs], data[0, useable_idxs]))\n",
    "        df = data.shape[1] - 1\n",
    "        print('Test statistics:')\n",
    "        print(\"M=%0.2f, t(%d)=%0.2f, p=%0.3f\" % (delta_mean,df,t,p))\n",
    "\n",
    "    \n",
    "# Create all of the plots\n",
    "def plot_parameter(parameter_type):\n",
    "\n",
    "    if parameter_type == 'SNR':\n",
    "        parameter_idx = 0\n",
    "    elif parameter_type == 'SFNR':\n",
    "        parameter_idx = 1        \n",
    "    elif parameter_type == 'FWHM':\n",
    "        parameter_idx = 2        \n",
    "    elif parameter_type == 'AR':\n",
    "        parameter_idx = 3         \n",
    "    elif parameter_type == 'MA':\n",
    "        parameter_idx = 4         \n",
    "            \n",
    "    # Make the box plot\n",
    "    plot_boxplot(real_noise_dicts[:, :, parameter_idx], simulated_noise_dicts_means[:, :, parameter_idx])\n",
    "    plt.title(parameter_type)\n",
    "    plt.xticks(np.asarray([1,2]), ['Real', 'Simulated'])\n",
    "    plt.savefig('../plots/%s_comparison.eps' % parameter_type, format='eps', dpi=100)\n",
    "    ylims = plt.ylim()\n",
    "\n",
    "\n",
    "def plot_persistence(barcode):\n",
    "\n",
    "    # Pull out all the features\n",
    "    births = barcode[:, 0]\n",
    "    deaths = barcode[:, 1]\n",
    "    betti = barcode[:, 2]\n",
    "\n",
    "    plt.plot(births[betti==1], deaths[betti==1], marker='x', linestyle='none')\n",
    "    plt.plot(births[betti==0], deaths[betti==0], marker='o', fillstyle='none', linestyle='none')\n",
    "\n",
    "    # Get the diagonal\n",
    "    all_vals = barcode[:,:2].flatten()\n",
    "    min_val = all_vals.min()\n",
    "    max_val = all_vals[np.argsort(all_vals)[-2]]  # So as not to get an inf\n",
    "\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], c='k')\n",
    "\n",
    "#     # Hide units\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#     plt.xlabel('Birth')\n",
    "#     plt.ylabel('Death')\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert steps into an appropriate string for printing\n",
    "def conv_steps(steps):\n",
    "    step_str = str(steps)\n",
    "    \n",
    "    # Remove offending characeters\n",
    "    step_str = step_str.replace('[', '')\n",
    "    step_str = step_str.replace(']', '')\n",
    "    step_str = step_str.replace(',', '')\n",
    "    \n",
    "    return step_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "short_style = '--'\n",
    "long_style = '-'\n",
    "very_long_style = '-.'\n",
    "low_nodes_color = np.asarray([143, 199, 54]) / 255\n",
    "mid_nodes_color = np.asarray([255, 93, 54]) / 255\n",
    "high_nodes_color = np.asarray([73, 94, 204]) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the range of parameters to be searched over\n",
    "signal_steps = [0.0, 0.25, 0.5, 0.75, 1.0, 5.0]\n",
    "event_types = [12, 15, 18]\n",
    "repetition_steps = [5, 10] \n",
    "isi = '5.0'\n",
    "deconvolution = 1 # Do you want to use the deconvolution version\n",
    "resample = 1 # Which resample do you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file names\n",
    "mask_name='../simulator_parameters/real_results/significant_mask.nii.gz'\n",
    "subject_root = '../simulated_data/node_brain_dist/'\n",
    "supersubject_root = '../simulated_data/supersubject_node_brain_dist/'\n",
    "nii = nb.load(mask_name)\n",
    "mask_vol = nii.get_data()\n",
    "mask_vec = mask_vol.reshape((np.prod(mask_vol.shape[0:3]))) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"check\"></a> \n",
    "## Check that the appropriate analyses have been run\n",
    "Do a quick check to ensure that the files you need for plotting here exist. This is an incomplete list but will suffice for the first few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First print the commands that need to be run (make sure you set the long partition appropriately) <a id=\"supersubject_script\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for signal in %s; do for events in %s; do for repetitions in %s; do sbatch -p $LONG_PARTITION ./code/supervisor_supersubject.sh elipse $signal [1,1] 1 $events $repetitions 1; done; done; done' % (conv_steps(signal_steps), conv_steps(event_types), conv_steps(repetition_steps)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check whether each file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('../validation_summary/real_noise_dicts.npy') == 0:\n",
    "    print('Cannot find files for ''Validation simulation'' script. Run `./code/supervisor_generate_noise_validation.sh` and then after that is finished, run `sbatch code/run_testing_noise_calc_noise_validation.sh`\\n')\n",
    "    \n",
    "if os.path.isfile('%s/%selipse_s-%s_1_1_t_5.0_1_1.0_%d_%d_1_resample-1.nii.gz' % (subject_root, 'sub-1_', signal_steps[0], repetition_steps[0], event_types[0])) == 0:\n",
    "    print('Cannot find files for generating the MDS plots (and distance matrices and persistence diagrams). Make sure have run at least `generate_node_brain_dist.py` for the conditions. The best way to do this is to do the supersubject analysis, as printed above\\n')\n",
    "\n",
    "if os.path.isfile('../searchlight_summary/signal_vs_flipped.txt') == 0 or os.path.isfile('../searchlight_summary/signal_proportion.txt')  == 0 or os.path.isfile('../searchlight_summary/flipped_proportion.txt')  == 0 or os.path.isfile('../searchlight_summary/signal_ratio.txt')  == 0 or os.path.isfile('../searchlight_summary/flipped_ratio.txt') == 0 :\n",
    "    print('Cannot find the summary of searchlight information. Make sure you have run the supervisor script, printed above, as well as the `run_test_signal_vol.sh` for each searchlight output')\n",
    "    print('Consider using the following command:\\nfiles=`ls simulated_data/searchlights/*`; for file in $files; do sbatch run_test_signal_vol.sh $file ./searchlight_summary/; done;\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the simulation accuracy<a id=\"validate\"></a> \n",
    "Using an approach that was used to validate the fmrisim package we check that the noise simulation is appropriate for this data.\n",
    "\n",
    "To run this analysis, first run `./code/supervisor_generate_noise_validation.sh` and then after that is finished, run `sbatch code/run_testing_noise_calc_noise_validation.sh`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the output of the fitting analyses and plot the heatmap of results\n",
    "\n",
    "# Identify file. paths\n",
    "testing_noise_calc_path = '../validation_summary/'\n",
    "\n",
    "# What is the file name\n",
    "real_noise_dicts = np.load(testing_noise_calc_path + 'real_noise_dicts.npy')\n",
    "simulated_noise_dicts = np.load(testing_noise_calc_path + 'simulated_noise_dicts.npy')\n",
    "simulated_noise_dicts_means = np.load(testing_noise_calc_path + 'simulated_noise_dicts_means.npy')\n",
    "simulated_noise_dicts_std = np.load(testing_noise_calc_path + 'simulated_noise_dicts_stds.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter('SNR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter('SFNR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter('FWHM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter('AR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mds\"></a> \n",
    "## Pull out the MDS representations from the fully simulated data\n",
    "This gives you a simple and easy way to visual the structure of the data in the signal voxels. \n",
    "\n",
    "This assumes that you have run `generate_node_brain_dist.py` for the relevant participants and conditions. In this case that is subject 1 and the supersubject analysis for several signal steps, event_types and repetitions per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subj = 1 # If zero then do the supersubject\n",
    "\n",
    "for nodes in event_types:\n",
    "    for repetitions_per_run in repetition_steps:\n",
    "\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        for counter, signal_size in enumerate(signal_steps):\n",
    "\n",
    "            # Set the subject name\n",
    "            if subj == 0:\n",
    "                subject_name = ''\n",
    "                file_root = supersubject_root\n",
    "            else:\n",
    "                subject_name = 'sub-%d_' % subj\n",
    "                file_root = subject_root\n",
    "\n",
    "            # Create a variable with a specific dp precision\n",
    "            signal_number = '%0.2f' % signal_size\n",
    "            if signal_number[-1] == '0':\n",
    "                signal_number = signal_number[:-1]\n",
    "\n",
    "            filename = '%s/%selipse_s-%s_1_1_t_5.0_1_1.0_%d_%d_%d_resample-%d.nii.gz' % (file_root, subject_name, signal_number, repetitions_per_run, nodes, deconvolution, resample)\n",
    "            plt.suptitle('reps-%d, nodes-%d, deconv-%d' % (repetitions_per_run, nodes, deconvolution))\n",
    "            \n",
    "            if os.path.exists(filename):\n",
    "                data_dist=average_node_brain_dist(filename, mask_vec)\n",
    "                \n",
    "                plt.subplot(3, len(signal_steps), counter + 1 )\n",
    "                graphs.make_mds(data_dist,\n",
    "                                dim=2,\n",
    "                                )\n",
    "                plt.axis('equal')\n",
    "                \n",
    "                plt.subplot(3, len(signal_steps), counter + 1 + len(signal_steps))\n",
    "                plt.imshow(data_dist)\n",
    "                plt.colorbar()\n",
    "                plt.axis('off')\n",
    "                plt.axis('equal')\n",
    "                \n",
    "                # Calculate the persistent homology of this distance matrix\n",
    "                barcode = test_graphs.make_persistence(data_dist)\n",
    "                \n",
    "                plt.subplot(3, len(signal_steps), counter + 1 + len(signal_steps) * 2)\n",
    "                plot_persistence(barcode)\n",
    "                \n",
    "                plt.xlabel('s-%0.2f' % signal_size)\n",
    "                \n",
    "        plt.savefig('../plots/example_mds_%snodes_%d_repetitions_%d.eps' % (subject_name, nodes, repetitions_per_run))\n",
    "        plt.savefig('../plots/example_mds_%snodes_%d_repetitions_%d.png' % (subject_name, nodes, repetitions_per_run))            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subj = 0 # If zero then do the supersubject\n",
    "\n",
    "for nodes in event_types:\n",
    "    for repetitions_per_run in repetition_steps:\n",
    "\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        for counter, signal_size in enumerate(signal_steps):\n",
    "\n",
    "            # Set the subject name\n",
    "            if subj == 0:\n",
    "                subject_name = ''\n",
    "                file_root = supersubject_root\n",
    "            else:\n",
    "                subject_name = 'sub-%d_' % subj\n",
    "                file_root = subject_root\n",
    "\n",
    "            # Create a variable with a specific dp precision\n",
    "            signal_number = '%0.2f' % signal_size\n",
    "            if signal_number[-1] == '0':\n",
    "                signal_number = signal_number[:-1]\n",
    "\n",
    "            filename = '%s/%selipse_s-%s_1_1_t_5.0_1_1.0_%d_%d_%d_resample-%d.nii.gz' % (file_root, subject_name, signal_number, repetitions_per_run, nodes, deconvolution, resample)\n",
    "\n",
    "            duration = (repetitions_per_run * nodes * 5 * 9) / 60  # How long in minutes is this experiment\n",
    "            plt.suptitle('reps-%d, nodes-%d, deconv-%d' % (repetitions_per_run, nodes, deconvolution))\n",
    "\n",
    "            if os.path.exists(filename):\n",
    "                data_dist=average_node_brain_dist(filename, mask_vec)\n",
    "                \n",
    "                plt.subplot(3, len(signal_steps), counter + 1 )\n",
    "               \n",
    "                graphs.make_mds(data_dist,\n",
    "                                dim=2,\n",
    "                                )\n",
    "                plt.axis('equal')\n",
    "                \n",
    "                plt.subplot(3, len(signal_steps), counter + 1 + len(signal_steps))\n",
    "                plt.imshow(data_dist)\n",
    "                plt.colorbar()\n",
    "                plt.axis('off')\n",
    "                plt.axis('equal')\n",
    "                \n",
    "                # Calculate the persistence diagram\n",
    "                barcode = test_graphs.make_persistence(data_dist)\n",
    "                \n",
    "                plt.subplot(3, len(signal_steps), counter + 1 + len(signal_steps) * 2)\n",
    "                plot_persistence(barcode)\n",
    "                \n",
    "                plt.xlabel('s-%0.2f' % signal_size)\n",
    "                \n",
    "                \n",
    "        plt.savefig('../plots/example_mds_%snodes_%d_repetitions_%d.eps' % (subject_name, nodes, repetitions_per_run))\n",
    "        plt.savefig('../plots/example_mds_%snodes_%d_repetitions_%d.png' % (subject_name, nodes, repetitions_per_run))            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"max_loop\"></a> \n",
    "## Make max loop signal plots\n",
    "Make the plot of the maximum loop persistence for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all of the summary statistics \n",
    "fid = open('../searchlight_summary/signal_vs_flipped.txt', 'r')\n",
    "file_txt = fid.readlines()\n",
    "fid.close()\n",
    "\n",
    "subject_data = {}\n",
    "supersubject_data = {}\n",
    "for line in file_txt:\n",
    "    \n",
    "    # What condition is it\n",
    "    condition = line[line.find('elipse'):line.find('_loop')]\n",
    "    \n",
    "    # What is the value\n",
    "    val = float(line[line.find(': ') + 2:line.find('\\n')])\n",
    "    \n",
    "    # Is it a supersubject or not?\n",
    "    if line.find('sub-') > -1:\n",
    "        \n",
    "        ppt = line[line.find('sub-'):line.find('_elipse')]\n",
    "        \n",
    "        # Add a dictionary if it doesn't exist\n",
    "        if condition not in subject_data:\n",
    "            subject_data[condition] = {}\n",
    "            \n",
    "        # Add to the list    \n",
    "        subject_data[condition][ppt] = val\n",
    "        \n",
    "    else:\n",
    "        supersubject_data[condition] = val\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "threshold = stats.t.ppf(0.9995, 441)\n",
    "\n",
    "# Cycle through all of the conditions\n",
    "for trials in [5, 10]:\n",
    "    if trials == 5:\n",
    "        line_style = short_style\n",
    "    elif trials == 10:\n",
    "        line_style = long_style\n",
    "    elif trials == 25:\n",
    "        line_style = very_long_style\n",
    "        \n",
    "    for nodes in [12, 15, 18]:\n",
    "        keys = []\n",
    "\n",
    "        if nodes == 12:\n",
    "            node_color = low_nodes_color\n",
    "        elif nodes == 15:\n",
    "            node_color = mid_nodes_color\n",
    "        elif nodes == 18:\n",
    "            node_color = high_nodes_color\n",
    "\n",
    "        keys = []\n",
    "        for s in signal_steps:\n",
    "\n",
    "            # Store the keys\n",
    "            keys += ['elipse_s-%s_1_1_t_%s_1_1.0_%s_%d_%d_resample-%d' % (str(s), isi, trials, nodes, deconvolution, resample)]\n",
    "\n",
    "        plot_summary_stats(subject_data, keys, line_style, node_color)\n",
    "\n",
    "\n",
    "plt.plot([0, len(signal_steps) - 1], [threshold, threshold], 'k--')\n",
    "plt.xticks(np.arange(len(signal_steps)), signal_steps)\n",
    "plt.ylabel('t stat')\n",
    "plt.xlabel('Percent signal change')\n",
    "plt.ylim([-10, 25])\n",
    "\n",
    "plt.savefig('../plots/subjectwise_max_loop_tstat.eps')\n",
    "plt.savefig('../plots/subjectwise_max_loop_tstat.png')            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Cycle through all of the conditions\n",
    "for trials in repetition_steps:\n",
    "    if trials == 5:\n",
    "        line_style = short_style\n",
    "    elif trials == 10:\n",
    "        line_style = long_style\n",
    "        \n",
    "    for nodes in event_types:\n",
    "        keys = []\n",
    "\n",
    "        if nodes == 12:\n",
    "            node_color = low_nodes_color\n",
    "        elif nodes == 15:\n",
    "            node_color = mid_nodes_color\n",
    "        elif nodes == 18:\n",
    "            node_color = high_nodes_color\n",
    "\n",
    "        keys = []\n",
    "        for s in signal_steps:\n",
    "\n",
    "            # Store the keys\n",
    "            keys += ['elipse_s-%s_1_1_t_%s_1_1.0_%s_%d_%d_resample-%d' % (str(s), isi, trials, nodes, deconvolution, resample)]\n",
    "\n",
    "        plot_summary_stats(supersubject_data, keys, line_style, node_color)\n",
    "\n",
    "plt.plot([0, len(signal_steps) - 1], [threshold, threshold], 'k--')\n",
    "plt.xticks(np.arange(len(signal_steps)), signal_steps)\n",
    "plt.ylabel('t stat')\n",
    "plt.xlabel('Percent signal change')\n",
    "plt.ylim([-10, 25])\n",
    "\n",
    "plt.savefig('../plots/supersubject_max_loop_tstat.eps')\n",
    "plt.savefig('../plots/supersubject_max_loop_tstat.png')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"proportion_loop\"></a> \n",
    "## Make the plots for the matching loop number\n",
    "Make the plots showing the frequency of getting exactly one loop (a single point in the 1-Dimensional persistence diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What method do you want to use to select the single point. 'proportion' means the number of searchlights with only one loop feature, 'ratio' means the number of loops above a threshold\n",
    "threshold_type = 'proportion'  \n",
    "\n",
    "# Load in all of the summary statistics \n",
    "fid = open('../searchlight_summary/signal_%s.txt' % threshold_type, 'r')\n",
    "file_txt = fid.readlines()\n",
    "fid.close()\n",
    "\n",
    "subject_signal = {}\n",
    "supersubject_signal = {}\n",
    "for line in file_txt:\n",
    "    \n",
    "    # What condition is it\n",
    "    condition = line[line.find('elipse'):line.find('_loop_counter')]\n",
    "    \n",
    "    # What is the value\n",
    "    val = float(line[line.find(': ') + 2:line.find('\\n')])\n",
    "    \n",
    "    # Is it a supersubject or not?\n",
    "    if line.find('sub-') > -1:\n",
    "        \n",
    "        ppt = line[line.find('sub-'):line.find('_elipse')]\n",
    "        \n",
    "        # Add a dictionary if it doesn't exist\n",
    "        if condition not in subject_signal:\n",
    "            subject_signal[condition] = {}\n",
    "            \n",
    "        # Add to the list    \n",
    "        subject_signal[condition][ppt] = val\n",
    "    else:\n",
    "        \n",
    "        if condition in supersubject_signal:\n",
    "            print('Condition already stored in the dict, not adding')\n",
    "        else:\n",
    "            supersubject_signal[condition] = val\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What method do you want to use to select the single point. 'proportion' means the number of searchlights with only one loop feature, 'ratio' means the number of loops above a threshold\n",
    "threshold_type = 'proportion'  \n",
    "\n",
    "# Load in all of the summary statistics \n",
    "fid = open('../searchlight_summary_test/signal_%s.txt' % threshold_type, 'r')\n",
    "file_txt = fid.readlines()\n",
    "fid.close()\n",
    "\n",
    "subject_signal = {}\n",
    "supersubject_signal = {}\n",
    "for line in file_txt:\n",
    "    \n",
    "    # What condition is it\n",
    "    condition = line[line.find('elipse'):line.find('_loop_counter')]\n",
    "    \n",
    "    # What is the value\n",
    "    val = float(line[line.find(': ') + 2:line.find('\\n')])\n",
    "    \n",
    "    # Is it a supersubject or not?\n",
    "    if line.find('sub-') > -1:\n",
    "        \n",
    "        ppt = line[line.find('sub-'):line.find('_elipse')]\n",
    "        \n",
    "        # Add a dictionary if it doesn't exist\n",
    "        if condition not in subject_signal:\n",
    "            subject_signal[condition] = {}\n",
    "            \n",
    "        # Add to the list    \n",
    "        subject_signal[condition][ppt] = val\n",
    "    else:\n",
    "        \n",
    "        if condition in supersubject_signal:\n",
    "            print('Condition already stored in the dict, not adding')\n",
    "            print(line)\n",
    "        else:\n",
    "            supersubject_signal[condition] = val\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all of the summary statistics \n",
    "fid = open('../searchlight_summary/flipped_%s.txt' % threshold_type, 'r')\n",
    "file_txt = fid.readlines()\n",
    "fid.close()\n",
    "\n",
    "subject_noise = {}\n",
    "supersubject_noise = {}\n",
    "for line in file_txt:\n",
    "    \n",
    "    # What condition is it\n",
    "    condition = line[line.find('elipse'):line.find('_loop')]\n",
    "    \n",
    "    # Check the general properties are correct\n",
    "    if condition.find('1_1_t_5.0_1_1.0') > 0:\n",
    "\n",
    "        # Pull out the rep number\n",
    "        if condition.find('_1.0_5_1') > 0:\n",
    "            rep_name = 'short'\n",
    "        elif condition.find('_1.0_10_1') > 0:\n",
    "            rep_name = 'long'\n",
    "        else:\n",
    "            rep_name = 'other_rep'   \n",
    "\n",
    "        # Pull out the node number\n",
    "        if condition.find('_12_1') > 0:\n",
    "            node_name = 'low'\n",
    "        elif condition.find('_15_1') > 0:\n",
    "            node_name = 'mid'\n",
    "        elif condition.find('_18_1') > 0:\n",
    "            node_name = 'high'\n",
    "        else:\n",
    "            node_name = 'other_node'\n",
    "\n",
    "        condition_name = rep_name + '_' + node_name\n",
    "\n",
    "        # What is the value\n",
    "        val = float(line[line.find(': ') + 2:line.find('\\n')])\n",
    "\n",
    "        # Is it a supersubject or not?\n",
    "        if line.find('sub-') > -1:\n",
    "            # Add a dictionary if it doesn't exist\n",
    "            if condition_name not in subject_noise:\n",
    "                subject_noise[condition_name] = []\n",
    "\n",
    "            subject_noise[condition_name] += [val]\n",
    "        else:\n",
    "            # Add a dictionary if it doesn't exist\n",
    "            if condition_name not in supersubject_noise:\n",
    "                supersubject_noise[condition_name] = []\n",
    "            supersubject_noise[condition_name] += [val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loaded in summary statistics\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Set the variables\n",
    "# Cycle through all of the conditions\n",
    "for trials in repetition_steps:\n",
    "    if trials == 5:\n",
    "        line_style = short_style\n",
    "    elif trials == 10:\n",
    "        line_style = long_style\n",
    "        \n",
    "    for nodes in event_types:\n",
    "        keys = []\n",
    "\n",
    "        if nodes == 12:\n",
    "            node_color = low_nodes_color\n",
    "        elif nodes == 15:\n",
    "            node_color = mid_nodes_color\n",
    "        elif nodes == 18:\n",
    "            node_color = high_nodes_color\n",
    "\n",
    "        keys = []\n",
    "        for s in signal_steps:\n",
    "\n",
    "            # Store the keys\n",
    "            keys += ['elipse_s-%s_1_1_t_%s_1_1.0_%s_%d_%d_resample-%d' % (str(s), isi, trials, nodes, deconvolution, resample)]\n",
    "\n",
    "        plot_summary_stats(subject_signal, keys, line_style, node_color)\n",
    "\n",
    "# Plot the noise pattern\n",
    "start_x_pos = 0.881\n",
    "step_x_pos = 0.019\n",
    "for node_name in [['low', low_nodes_color], ['mid', mid_nodes_color], ['high', high_nodes_color]]:\n",
    "    for rep_name in [['short_', '/'  * 5], ['long_', None]]:\n",
    "        \n",
    "        # Plot this condition\n",
    "        plt.axhspan(np.min(subject_noise[rep_name[0] + node_name[0]]), np.max(subject_noise[rep_name[0] + node_name[0]]), xmin=start_x_pos, xmax=start_x_pos + step_x_pos, alpha=0.1, color=node_name[1], hatch = rep_name[1])\n",
    "        \n",
    "        start_x_pos += step_x_pos + 0.001\n",
    "\n",
    "# Plot the graph features\n",
    "plt.xticks(np.arange(len(signal_steps)), signal_steps)\n",
    "plt.ylabel('Prop. 1 loop')\n",
    "plt.xlabel('Percent signal change')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 6])\n",
    "\n",
    "plt.savefig('../plots/subjectwise_%s.eps' % threshold_type)\n",
    "plt.savefig('../plots/subjectwise_%s.png' % threshold_type)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "\n",
    "# Cycle through all of the conditions\n",
    "for trials in repetition_steps:\n",
    "    if trials == 5:\n",
    "        line_style = short_style\n",
    "    elif trials == 10:\n",
    "        line_style = long_style\n",
    "        \n",
    "    for nodes in event_types:\n",
    "        keys = []\n",
    "\n",
    "        if nodes == 12:\n",
    "            node_color = low_nodes_color\n",
    "        elif nodes == 15:\n",
    "            node_color = mid_nodes_color\n",
    "        elif nodes == 18:\n",
    "            node_color = high_nodes_color\n",
    "\n",
    "        for s in signal_steps:\n",
    "\n",
    "            # Store the keys\n",
    "            keys += ['elipse_s-%s_1_1_t_%s_1_1.0_%s_%d_%d_resample-%d' % (str(s), isi, trials, nodes, deconvolution, resample)]\n",
    "\n",
    "        plot_summary_stats(supersubject_signal, keys, line_style, node_color)\n",
    "\n",
    "# Plot the noise pattern\n",
    "start_x_pos = 0.881\n",
    "step_x_pos = 0.019\n",
    "for node_name in [['low', low_nodes_color], ['mid', mid_nodes_color], ['high', high_nodes_color]]:\n",
    "    for rep_name in [['short_', '/' * 5], ['long_', None]]:\n",
    "        \n",
    "        # Plot this condition\n",
    "        plt.axhspan(np.min(supersubject_noise[rep_name[0] + node_name[0]]), np.max(supersubject_noise[rep_name[0] + node_name[0]]), xmin=start_x_pos, xmax=start_x_pos + step_x_pos, alpha=0.1, color=node_name[1], hatch = rep_name[1])\n",
    "        \n",
    "        start_x_pos += step_x_pos + 0.001        \n",
    "        \n",
    "# Plot the graph features\n",
    "plt.xticks(np.arange(len(signal_steps)), signal_steps)\n",
    "plt.ylabel('Prop. 1 loop')\n",
    "plt.xlabel('Percent signal change')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 6])\n",
    "\n",
    "plt.savefig('../plots/supersubject_%s.eps'% threshold_type)\n",
    "plt.savefig('../plots/supersubject_%s.png' % threshold_type)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ratio_loop\"></a> \n",
    "## Search over different thresholds for TDA\n",
    "As per the supplemental analysis, examine what is the effect of different ratio thresholds on whether one loop is identified. A value of 2 means that the persistence of the largest loop must be at least 2x the persistence of the next largest loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_type = 'ratio'\n",
    "ratio_steps = [1.1, 1.2, 1.4, 1.8, 2.6, 4.2, 7.4, 13.8]\n",
    "s = '0.75'\n",
    "resample = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have run the [supervisor script](#supersubject_script) , you can run the searchlight analysis selectively for different thresholds. Use the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for signal in %s; do for events in %s; do for repetitions in %s; do for ratio in %s; do sbatch ./code/supervisor_searchlight.sh simulated_data/searchlights/ elipse_s-${signal}_1_1_t_5.0_1_1.0_${events}_${repetitions}_1_resample-1 loop_ratio_${ratio}; done; done; done' % (conv_steps(s), conv_steps(event_types), conv_steps(repetition_steps), conv_steps(ratio_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, run the following line to summarise these volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('files=`ls simulated_data/searchlights/*loop_ratio_*`; for file in $files; do sbatch run_test_signal_vol.sh $file ./searchlight_summary/; done;\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all of the summary statistics \n",
    "fid = open('../searchlight_summary/signal_%s.txt' % threshold_type, 'r')\n",
    "file_txt = fid.readlines()\n",
    "fid.close()\n",
    "\n",
    "subject_signal = {}\n",
    "supersubject_signal = {}\n",
    "for ratio_threshold in ratio_steps:\n",
    "\n",
    "    for line in file_txt:\n",
    "\n",
    "        # What condition is it\n",
    "        start_idx = line.find('elipse')\n",
    "        end_idx = line.find('_loop_ratio_%0.1f' % ratio_threshold)\n",
    "        \n",
    "        # Ensure that it is the correct condition\n",
    "        if start_idx > -1 and end_idx > -1:\n",
    "            \n",
    "            condition = line[start_idx:end_idx]\n",
    "\n",
    "            condition += '_loop_ratio_%0.1f' % ratio_threshold\n",
    "\n",
    "            # What is the value\n",
    "            val = float(line[line.find(': ') + 2:line.find('\\n')])\n",
    "\n",
    "            # Is it a supersubject or not?\n",
    "            if line.find('sub-') > -1:\n",
    "\n",
    "                ppt = line[line.find('sub-'):line.find('_elipse')]\n",
    "\n",
    "                # Add a dictionary if it doesn't exist\n",
    "                if condition not in subject_signal:\n",
    "                    subject_signal[condition] = {}\n",
    "\n",
    "                # Add to the list    \n",
    "                subject_signal[condition][ppt] = val\n",
    "\n",
    "            else:\n",
    "                supersubject_signal[condition] = val \n",
    "   \n",
    "\n",
    "fid = open('../searchlight_summary/flipped_%s.txt' % threshold_type, 'r')\n",
    "file_txt = fid.readlines()\n",
    "fid.close()\n",
    "\n",
    "subject_noise = {}\n",
    "supersubject_noise = {}\n",
    "for ratio_threshold in ratio_steps:\n",
    "\n",
    "    for line in file_txt:\n",
    "\n",
    "        # What condition is it\n",
    "        start_idx = line.find('elipse')\n",
    "        end_idx = line.find('_loop_ratio_%0.1f' % ratio_threshold)\n",
    "        \n",
    "        # Ensure that it is the correct condition\n",
    "        if start_idx > -1 and end_idx > -1:\n",
    "        \n",
    "            condition = line[start_idx:end_idx]\n",
    "\n",
    "            condition += '_loop_ratio_%0.1f' % ratio_threshold\n",
    "\n",
    "            # What is the value\n",
    "            val = float(line[line.find(': ') + 2:line.find('\\n')])\n",
    "\n",
    "            # Is it a supersubject or not?\n",
    "            if line.find('sub-') > -1:\n",
    "\n",
    "                ppt = line[line.find('sub-'):line.find('_elipse')]\n",
    "\n",
    "                # Add a dictionary if it doesn't exist\n",
    "                if condition not in subject_noise:\n",
    "                    subject_noise[condition] = {}\n",
    "\n",
    "                # Add to the list    \n",
    "                subject_noise[condition][ppt] = val\n",
    "            else:\n",
    "                supersubject_noise[condition] = val\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cycle through all of the conditions\n",
    "\n",
    "for trials in repetition_steps:\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    if trials == 5:\n",
    "        line_style = short_style\n",
    "    elif trials == 10:\n",
    "        line_style = long_style\n",
    "        \n",
    "    for nodes in event_types:\n",
    "        keys = []\n",
    "\n",
    "        if nodes == 12:\n",
    "            node_color = low_nodes_color\n",
    "        elif nodes == 15:\n",
    "            node_color = mid_nodes_color\n",
    "        elif nodes == 18:\n",
    "            node_color = high_nodes_color\n",
    "\n",
    "        for ratio in ratio_steps:\n",
    "\n",
    "            # Store the keys\n",
    "            keys += ['elipse_s-%s_1_1_t_%s_1_1.0_%s_%d_%d_resample-%d_loop_ratio_%0.1f' % (s, isi, trials, nodes, deconvolution, resample, ratio)]\n",
    "        \n",
    "\n",
    "        sig_data = plot_summary_stats(subject_signal, keys, long_style, node_color)[0]\n",
    "        noise_data = plot_summary_stats(subject_noise, keys, ':', node_color)[0]\n",
    "        \n",
    "        # Plot the difference between signal and noise\n",
    "        plt.plot(np.asarray(sig_data) - np.asarray(noise_data), c=node_color, linewidth=5)\n",
    "\n",
    "    # Plot for this set of conditions\n",
    "    plt.xticks(np.arange(len(ratio_steps)), ratio_steps)  \n",
    "    plt.ylabel('Prop. 1 loop')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('Ratio threshold') \n",
    "    \n",
    "    # Save before adding some elements you don't want in the final fig\n",
    "    plt.savefig('../plots/subject_%s_repetitons-%d.eps'% (threshold_type, trials))\n",
    "    plt.savefig('../plots/subject_%s_repetitons-%d.png'% (threshold_type, trials))\n",
    "    \n",
    "    plt.title('PSC-%s_trials-%s_nodes-%d' % (s, trials, nodes))  \n",
    "    plt.legend(['Signal', 'Noise', 'Diff'] * 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"searchlights_in_ROI\"></a> \n",
    "## How many searchlights contain the ROI\n",
    "Run a (fast) searchlight in which you count how many voxels of the signal ROI are in the searchlight. This can tell you how many searchlights you run will contain signal voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import nibabel\n",
    "from brainiak.searchlight.searchlight import Searchlight\n",
    "import os\n",
    "import sys\n",
    "from test_graph_structure import sl_kernel\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Load in the signal mask\n",
    "mask_file = '/gpfs/milgram/project/turk-browne/users/ce325/event_related_fmri_tda/simulator_parameters/real_results/significant_mask.nii.gz'\n",
    "\n",
    "def sl_kernel(data, mask, myrad, bcvar):\n",
    "    # How many mask voxels are there\n",
    "    return data[0].sum()\n",
    "\n",
    "# Load in the signal_mask\n",
    "nii = nibabel.load(mask_file)\n",
    "dimsize=nii.header.get_zooms()\n",
    "signal_mask = nii.get_data()\n",
    "\n",
    "# Look over all voxels in the brain (slow but easy)\n",
    "mask = np.ones(signal_mask.shape)\n",
    "\n",
    "signal_mask = signal_mask.reshape((signal_mask.shape[0], signal_mask.shape[1], signal_mask.shape[2], 1))\n",
    "\n",
    "# Create searchlight object\n",
    "sl = Searchlight(sl_rad=1, max_blk_edge=5)\n",
    "\n",
    "# Distribute data to processes\n",
    "sl.distribute([signal_mask], mask)\n",
    "sl.broadcast(None)\n",
    "\n",
    "# Run clusters\n",
    "sl_outputs = sl.run_searchlight(sl_kernel)\n",
    "\n",
    "sl_outputs = sl_outputs.astype('double')\n",
    "sl_outputs[np.isnan(sl_outputs)] = 0\n",
    "\n",
    "print('Searchlights with at least one voxel in the signal ROI', (sl_outputs >= 1).sum())\n",
    "print('Searchlights with at least ten voxel in the signal ROI', (sl_outputs >= 10).sum())\n",
    "print('Searchlights with all 27 voxel in the signal ROI', (sl_outputs == 27).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"figure_1\"></a> \n",
    "## Generate the elements for the methods figure in the manuscript\n",
    "This figure shows the topological structure being inserted in to the brain, the ROI containing signal, an example distance matrix and an example persistence diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords=graphs.elipse(nodes=12, x_coef=1, y_coef=1)\n",
    "dist=graphs.coord2dist(coords)\n",
    "plt.figure()\n",
    "graphs.make_mds(dist,\n",
    "                dim=2,\n",
    "                )\n",
    "plt.axis('off');\n",
    "plt.savefig('../plots/example_loop.eps')\n",
    "plt.savefig('../plots/example_loop.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"figure_1_slice\"></a> \n",
    "### Plot a brain slice\n",
    "Take a slice of the participant data and plot it. \n",
    "\n",
    "To make this participant you need to output the raw simulated data, which is not stored when you run with resampling on. Hence you need to run the following line:\n",
    "\n",
    "`sbatch -p $LONG_PARTITION ./code/supervisor_supersubject.sh elipse 5.0 [1,1] 0 5 12 1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_num = '1'\n",
    "participant_condition = 'elipse_s-5.0_1_1_t_5.0_1_1.0_5_12_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../simulated_data/nifti/sub-%s_r1_%s.nii.gz' % (participant_num, participant_condition)\n",
    "\n",
    "nii = nb.load(filename)\n",
    "vol = nii.get_data()\n",
    "\n",
    "# Pull out the slices to show\n",
    "x_idx = 11\n",
    "brain_slice = np.rot90(vol[x_idx, :, :, 0], 1)\n",
    "mask_slice = np.rot90(mask_vol[x_idx, :, :], 1)\n",
    "\n",
    "# Find an example searchlight\n",
    "y_idx = np.where(mask_slice == 1)[0][10]\n",
    "z_idx = np.where(mask_slice == 1)[1][10]\n",
    "\n",
    "# Create a searchlight mask\n",
    "searchlight_slice = np.zeros(mask_slice.shape)\n",
    "searchlight_slice[y_idx - 1:y_idx + 2, z_idx - 1:z_idx + 2] = 1\n",
    "\n",
    "# Set the range to 1\n",
    "brain_slice /= brain_slice.max()\n",
    "mask_slice /= mask_slice.max()\n",
    "\n",
    "# Construct RGB version of grey-level image\n",
    "img_rgb = np.dstack((brain_slice, brain_slice, brain_slice))\n",
    "\n",
    "mask_rgb = np.zeros(img_rgb.shape)\n",
    "mask_rgb[:, :, 2] = mask_slice\n",
    "\n",
    "searchlight_rgb = np.zeros(img_rgb.shape)\n",
    "searchlight_rgb[:, :, 1] = searchlight_slice\n",
    "\n",
    "# Convert the input image and color mask to Hue Saturation Value (HSV)\n",
    "# colorspace\n",
    "img_hsv = colors.rgb_to_hsv(img_rgb)\n",
    "mask_hsv = colors.rgb_to_hsv(mask_rgb)\n",
    "searchlight_hsv = colors.rgb_to_hsv(searchlight_rgb)\n",
    "\n",
    "# Replace the hue and saturation of the original image\n",
    "# with that of the color mask\n",
    "alpha = 0.7\n",
    "img_hsv[:, :, 0] = mask_hsv[:, :, 0]\n",
    "img_hsv[:, :, 1] = mask_hsv[:, :, 1] * alpha\n",
    "\n",
    "# Super impose the searchlight\n",
    "roi_hue = np.unique(searchlight_hsv[:, :, 0])[1]\n",
    "roi_sat = np.unique(searchlight_hsv[:, :, 1])[1]\n",
    "idxs = np.where(searchlight_hsv[:, :, 0] == roi_hue)\n",
    "\n",
    "img_hsv[idxs[0], idxs[1], 0] = roi_hue\n",
    "img_hsv[idxs[0], idxs[1], 1] = roi_sat\n",
    "\n",
    "# Convert back into rgb\n",
    "img_masked = colors.hsv_to_rgb(img_hsv)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img_masked)\n",
    "plt.axis('off')\n",
    "\n",
    "# Example brain\n",
    "plt.savefig('../plots/example_brain_searchlight.eps')\n",
    "plt.savefig('../plots/example_brain_searchlight.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"figure_1_dist\"></a> \n",
    "### Plot a distrance matrix\n",
    "Load in the distance matrix of a participant, average all of the voxels in the signal ROI and then plot the outcine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../simulated_data/node_brain_dist//sub-%s_%s.nii.gz' % (participant_num, participant_condition)\n",
    "\n",
    "data_dist = average_node_brain_dist(filename, mask_vec)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(data_dist)\n",
    "plt.axis('off')\n",
    "plt.savefig('../plots/example_distance_mat.eps')\n",
    "plt.savefig('../plots/example_distance_mat.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"figure_1_PH\"></a> \n",
    "### Run persistent homology on a searchlight voxel and plot it\n",
    "\n",
    "Get the persistence diagram from a specific ROI that is specified by the searchlight above\n",
    "\n",
    "In case the module for TDA is not easily loaded into your notebooks, use the script as specified below to run persistent homology outside of the notebook and then load in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example barcode\n",
    "input_file = 'simulated_data/node_brain_dist//sub-%s_%s.nii.gz' % (participant_num, participant_condition) # Use path relative to the base\n",
    "output_file = 'simulated_data/example_barcodes/sub-%s_%s_coord.npy' % (participant_num, participant_condition) # Use path relative to the base\n",
    "coordinates = '[%d,%d,%d]' % (x_idx,y_idx,z_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$input_file\" \"$coordinates\" \"$output_file\"\n",
    "cd ../; ./code/run_TDA_coordinate.sh $1 $2 $3; cd code/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the barcode\n",
    "barcode = np.load('../' + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plot_persistence(barcode)\n",
    "plt.savefig('../plots/example_persistence_diagram.png')\n",
    "plt.savefig('../plots/example_persistence_diagram.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"figure_S4\"></a> \n",
    "## Plot example signal activity and signal plus noise\n",
    "Run a version of the pipeline using only one run and storing useful outputs for plotting. Then use this data to make examples of the signal and noise as it is transformed between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the signal parameters\n",
    "participant_counter = 1  # Determines what noise you use\n",
    "signal_structure = 'elipse' # What is the shape to be inserted (elipse is not necessarily an elipse, depends on the signal properties)\n",
    "signal_properties = [1, 1]  # In the case of an elipse, what is the width and height\n",
    "signal_magnitude = signal_steps[-2] # What is the PSC for each voxel\n",
    "num_nodes = event_types[0] # How many nodes are you simulating\n",
    "repetitions_per_run = repetition_steps[0] # How many events per run are you simulating?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import nibabel\n",
    "import matplotlib.pyplot as plt\n",
    "import generate_graph_structure as graphs\n",
    "import copy\n",
    "import sys\n",
    "from scipy.stats import zscore\n",
    "from sklearn import linear_model\n",
    "from brainiak.utils import fmrisim as sim\n",
    "import logging\n",
    "import os\n",
    "from nilearn.image import smooth_img\n",
    "np.seterr(divide='ignore')  # Not relevant for this script\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Concatenate the timing information needed\n",
    "timing_properties = [5.0,1,1.0,repetitions_per_run,num_nodes,1]\n",
    "resample = 1\n",
    "\n",
    "# If it is not a resample, do you want to save some plots\n",
    "save_signal_func = 0  # Save the plot of the signal function?\n",
    "save_functional = 1  # Do you want to save the functional (with node_brain)\n",
    "\n",
    "# Inputs for generate_signal\n",
    "temporal_res = 100  # How many samples per second are there for timing files\n",
    "tr_duration = 2  # Force this value\n",
    "fwhm = 0 # What is the fwhm of the smoothing kernel\n",
    "all_pos = 1  # Assume all activation is positive\n",
    "\n",
    "# Set analysis parameters\n",
    "hrf_lag = 2 # How many TRs is the event offset by?\n",
    "zscore_time = 1  # Do you want to z score each voxel in time\n",
    "zscore_volume = 0  # Do you want to z score a volume\n",
    "\n",
    "# What are the timing properties\n",
    "minimum_isi = timing_properties[0]\n",
    "randomise_timing = timing_properties[1]\n",
    "event_durations = [timing_properties[2]]  # Assume the events are 1s long\n",
    "repetitions_per_run = timing_properties[3]  # How many nodes are you simulating\n",
    "nodes = timing_properties[4]  # How many nodes are to be created\n",
    "deconvolution = timing_properties[5] # Do you want to use the deconvolution method for aggregating across events\n",
    "\n",
    "\n",
    "## Set definitions\n",
    "\n",
    "def regress_voxels(voxel_time,\n",
    "                   design,\n",
    "                   intercept=0,\n",
    "                   ):\n",
    "    # Takes in a voxel by time matrix and a TR by condition design matrix.\n",
    "    # Assume that the timecourse used to create the design matrix was mean\n",
    "    # centred\n",
    "    \n",
    "    # Calculate the coefficients for each condition\n",
    "    voxels_reg = np.zeros((voxel_time.shape[0], design.shape[1] + intercept))\n",
    "    voxels_err = np.zeros((voxel_time.shape[0], 1))\n",
    "    voxels_r2 = np.zeros((voxel_time.shape[0], 1))\n",
    "    ols = linear_model.LinearRegression()\n",
    "    for voxel_counter in list(range(0, voxel_time.shape[0])):\n",
    "\n",
    "        # Get this voxel\n",
    "        voxel = voxel_time[voxel_counter, :]\n",
    "\n",
    "        # If this is more than a single column design matrix go\n",
    "        if design.shape[1] > 1:\n",
    "            if np.all(np.isnan(voxel)) == 0:\n",
    "\n",
    "                # What are the coefficients of each feature in the regression\n",
    "                fit = ols.fit(design, voxel)\n",
    "                coefs = fit.coef_\n",
    "\n",
    "                # Calculate an R squared between the predicted response (based\n",
    "                # on the coefficients) and the observed response\n",
    "                predicted = np.mean((design * coefs) + fit.intercept_, 1)\n",
    "                r2 = np.corrcoef(voxel, predicted)[0, 1] ** 2\n",
    "\n",
    "                # Do you want to also add the intercept to the output?\n",
    "                if intercept == 1:\n",
    "                    voxels_reg[voxel_counter, :] = np.append(coefs, fit.intercept_)\n",
    "                else:\n",
    "                    voxels_reg[voxel_counter, :] = coefs\n",
    "\n",
    "                voxels_err[voxel_counter, 0] = np.sqrt(np.sum((voxel - predicted) ** 2))\n",
    "                voxels_r2[voxel_counter, 0] = r2\n",
    "\n",
    "            else:\n",
    "                voxels_reg[voxel_counter, :] = np.nan\n",
    "                voxels_err[voxel_counter, 0] = np.nan\n",
    "                voxels_r2[voxel_counter, 0] = np.nan\n",
    "        else:\n",
    "\n",
    "            # Correlate the voxels and the design\n",
    "            voxels_r2[voxel_counter, 0] = np.corrcoef(voxel, design[:,\n",
    "                                                             0])[0, 1]\n",
    "\n",
    "    # # Convert all of the nans\n",
    "    # voxels_reg[np.isnan(voxels_reg)] = 0\n",
    "\n",
    "    # Output\n",
    "    if design.shape[1] > 1:\n",
    "        return voxels_reg, voxels_r2, voxels_err\n",
    "    else:\n",
    "        return voxels_r2\n",
    "\n",
    "\n",
    "signal_properties_str = str(signal_properties)\n",
    "signal_properties_str = signal_properties_str.replace('[', '_').replace(',',\n",
    "                                                                      '_').replace(']', '').replace(' ', '')\n",
    "\n",
    "timing_properties_str = str(timing_properties)\n",
    "timing_properties_str = timing_properties_str.replace('[', '_').replace(',',\n",
    "                                                                      '_').replace(']', '').replace(' ', '')\n",
    "\n",
    "# What is the participant name with these parameters (change from default)\n",
    "effect_name = '_' + signal_structure\n",
    "effect_name = effect_name + '_s-' + str(signal_magnitude)\n",
    "effect_name = effect_name + signal_properties_str\n",
    "effect_name = effect_name + '_t' + timing_properties_str\n",
    "\n",
    "# Only add this at the end\n",
    "if resample > 0:\n",
    "    effect_name = effect_name + '_resample-' + str(resample)\n",
    "\n",
    "# Specify the paths and names\n",
    "parameters_path = '../simulator_parameters/'\n",
    "simulated_data_path = '../simulated_data/'\n",
    "timing_path = parameters_path + 'timing/'\n",
    "\n",
    "participant = 'sub-' + str(participant_counter + 1)\n",
    "SigMask = parameters_path + '/real_results/significant_mask.nii.gz'\n",
    "savename = participant + effect_name\n",
    "node_brain_save = simulated_data_path + '/node_brain/' + savename + \\\n",
    "                  '.nii.gz'\n",
    "\n",
    "# Load data\n",
    "print('Loading ' + participant)\n",
    "\n",
    "# Load significant voxels\n",
    "nii = nibabel.load(SigMask)\n",
    "signal_mask = nii.get_data()\n",
    "\n",
    "dimsize = nii.header.get_zooms()  # x, y, z and TR size\n",
    "\n",
    "if signal_structure == 'community_structure':\n",
    "\n",
    "    # Load the timing information\n",
    "    nodes = 15  # overwrite\n",
    "    onsets_runs = np.load(timing_path + participant + '.npy')\n",
    "\n",
    "else:\n",
    "    # Generate the timing for this participant\n",
    "    base_runs = np.load(timing_path + participant + '.npy')\n",
    "    runs = len(base_runs)\n",
    "    onsets_runs = [-1] * runs\n",
    "    for run_counter in list(range(runs)):\n",
    "        \n",
    "        # If the number of repetitions per run is zero then use Schapiro's data to calculate it\n",
    "        if repetitions_per_run == 0:\n",
    "            \n",
    "            # How many events in this run for this participant\n",
    "            events = sum(len(event) for event in base_runs)\n",
    "\n",
    "            # How many events per node for this run?\n",
    "            repetitions_per_run = int(events / nodes)\n",
    "        \n",
    "        current_time = 0  # Initialize\n",
    "        onsets = [-1] * nodes  # Ignore first entry\n",
    "        for hamilton_counter in list(range(0, repetitions_per_run)):\n",
    "\n",
    "            # Pick on this hamilton the starting node and the direction\n",
    "            node = np.random.randint(0, nodes - 1)\n",
    "            direction = np.random.choice([-1, 1], 1)[0]\n",
    "\n",
    "            # Loop through the events\n",
    "            for node_counter in list(range(0, nodes)):\n",
    "\n",
    "                # Append this time\n",
    "                if np.all(onsets[node] == -1):\n",
    "                    onsets[node] = np.array(current_time)\n",
    "                else:\n",
    "                    onsets[node] = np.append(onsets[node],\n",
    "                                             current_time)\n",
    "\n",
    "                # What is the isi?\n",
    "                isi = minimum_isi + (np.random.randint(3) * tr_duration)\n",
    "\n",
    "                # Increment the time\n",
    "                current_time += event_durations[0] + isi\n",
    "\n",
    "                # Update the node\n",
    "                node += direction\n",
    "                if node >= nodes:\n",
    "                    node = 0\n",
    "                elif node < 0:\n",
    "                    node = (nodes - 1)\n",
    "\n",
    "        # Store the runs\n",
    "        print('There are %d onsets in run %d' % (len(onsets), run_counter))\n",
    "        onsets_runs[run_counter] = onsets\n",
    "\n",
    "# How many runs are there?\n",
    "runs = 1\n",
    "\n",
    "# Generate the indexes of all voxels that will contain signal\n",
    "vector_size = int(signal_mask.sum())\n",
    "\n",
    "# Find all the indices that are significant\n",
    "idx_list = np.where(signal_mask == 1)\n",
    "\n",
    "idxs = np.zeros([vector_size, 3])\n",
    "for idx_counter in list(range(0, len(idx_list[0]))):\n",
    "    idxs[idx_counter, 0] = idx_list[0][idx_counter]\n",
    "    idxs[idx_counter, 1] = idx_list[1][idx_counter]\n",
    "    idxs[idx_counter, 2] = idx_list[2][idx_counter]\n",
    "\n",
    "idxs = idxs.astype('int8')    \n",
    "    \n",
    "# What voxels are they\n",
    "dimensions = signal_mask.shape\n",
    "\n",
    "# Cycle through the runs and generate the data\n",
    "node_brain = np.zeros([dimensions[0], dimensions[1], dimensions[2],\n",
    "                       nodes], dtype='double')  # Preset\n",
    "\n",
    "# Generate the graph structure (based on the ratio)\n",
    "if signal_structure == 'community_structure':\n",
    "    signal_coords = graphs.community_structure(signal_properties[0],\n",
    "                                              )\n",
    "elif signal_structure == 'parallel_rings':\n",
    "    signal_coords = graphs.parallel_rings(nodes,\n",
    "                                          signal_properties[0],\n",
    "                                          signal_properties[1],\n",
    "                                         )\n",
    "elif signal_structure == 'figure_eight':\n",
    "    signal_coords = graphs.figure_eight(nodes,\n",
    "                                        signal_properties[0],\n",
    "                                        signal_properties[1],\n",
    "                                       )\n",
    "elif signal_structure == 'chain_rings':\n",
    "    signal_coords = graphs.chain_rings(nodes,\n",
    "                                       signal_properties[0],\n",
    "                                       signal_properties[1],\n",
    "                                      )\n",
    "elif signal_structure == 'dispersed_clusters':\n",
    "    signal_coords = graphs.dispersed_clusters(nodes,\n",
    "                                              signal_properties[0],\n",
    "                                              signal_properties[1],\n",
    "                                              signal_properties[2],\n",
    "                                             )\n",
    "elif signal_structure == 'tendrils':\n",
    "    signal_coords = graphs.tendrils(nodes,\n",
    "                                    signal_properties[0],\n",
    "                                    signal_properties[1],\n",
    "                                   )\n",
    "elif signal_structure == 'elipse':\n",
    "    signal_coords = graphs.elipse(nodes,\n",
    "                                  signal_properties[0],\n",
    "                                  signal_properties[1],\n",
    "                                 )\n",
    "\n",
    "\n",
    "# Perform an orthonormal transformation of the data\n",
    "if vector_size > signal_coords.shape[1]:\n",
    "    signal_coords = graphs.orthonormal_transform(vector_size,\n",
    "                                                      signal_coords)\n",
    "\n",
    "# Make a backup for later    \n",
    "signal_coords_ortho = np.copy(signal_coords)    \n",
    "    \n",
    "# Do you want these coordinates to be all positive? This means that\n",
    "# these coordinates are represented as different magnitudes of\n",
    "# activation\n",
    "if all_pos == 1:\n",
    "    mins = np.abs(np.min(signal_coords, 0))\n",
    "    for voxel_counter in list(range(0, len(mins))):\n",
    "        signal_coords[:, voxel_counter] += mins[voxel_counter]\n",
    "\n",
    "# Bound the value to have a max of 1 so that the signal magnitude is more interpretable\n",
    "signal_coords /= np.max(signal_coords)\n",
    "\n",
    "# Make a backup for later    \n",
    "signal_coords_all_pos = np.copy(signal_coords)    \n",
    "\n",
    "# Get run specific names\n",
    "template_name = parameters_path + 'template/' + participant + '_r' + \\\n",
    "                str(run_counter) + '.nii.gz'\n",
    "noise_dict_name = parameters_path + 'noise_dict/' + participant + '_r' + str(run_counter) + \\\n",
    "                  '.txt'\n",
    "nifti_save = simulated_data_path + 'nifti/' + participant + '_r' + str(run_counter)\\\n",
    "             + effect_name + '.nii.gz'\n",
    "signal_func_save = './plots/' + participant + '_r' +\\\n",
    "                   str(run_counter) + effect_name + '.png'\n",
    "\n",
    "# Load the template (not yet scaled\n",
    "nii = nibabel.load(template_name)\n",
    "template = nii.get_data()  # Takes a while\n",
    "\n",
    "# Create the mask and rescale the template\n",
    "mask, template = sim.mask_brain(template,\n",
    "                                mask_self=True,\n",
    "                                )\n",
    "\n",
    "# Make sure all the signal voxels are within the mask\n",
    "signal_mask_run = signal_mask * mask\n",
    "\n",
    "# Pull out the onsets for this participant (copy it so you don't alter it)\n",
    "onsets = copy.deepcopy(onsets_runs[run_counter - 1])\n",
    "\n",
    "# Do you want to randomise the onsets (so that the events do not have a\n",
    "# fixed order)\n",
    "if randomise_timing == 1:\n",
    "\n",
    "    # Extract all the timing information\n",
    "    onsets_all = onsets[0]\n",
    "    for node_counter in list(range(1, nodes)):\n",
    "        onsets_all = np.append(onsets_all, onsets[node_counter])\n",
    "\n",
    "    print('Randomizing timing across nodes')\n",
    "\n",
    "    # Shuffle the onsets\n",
    "    np.random.shuffle(onsets_all)\n",
    "\n",
    "    # Insert the shuffled onsets back in\n",
    "    for node_counter in list(range(0, nodes)):\n",
    "        node_num = len(onsets[node_counter])\n",
    "        onsets[node_counter] = np.sort(onsets_all[0:node_num])\n",
    "        onsets_all = onsets_all[node_num:] # Remove the onsets\n",
    "\n",
    "# If you want to use different timing then take the order of the data\n",
    "# and then create a new timecourse\n",
    "if minimum_isi > 1:\n",
    "\n",
    "    # Iterate through all the onset values\n",
    "    onsets_all = []  # Reset\n",
    "    for node_counter in list(range(0, nodes)):\n",
    "        onsets_all = np.append(onsets_all, onsets[node_counter])\n",
    "\n",
    "        # Take the idx of all of the elements in onset all\n",
    "        Idxs = np.zeros((len(onsets[node_counter]), 2))\n",
    "\n",
    "        Idxs[:, 0] = [node_counter] * len(onsets[node_counter])\n",
    "        Idxs[:, 1] = list(range(0, len(onsets[node_counter])))\n",
    "\n",
    "        # Append the indexes\n",
    "        if node_counter == 0:\n",
    "            onset_idxs = Idxs\n",
    "        else:\n",
    "            onset_idxs = np.concatenate((onset_idxs, Idxs))\n",
    "\n",
    "    # Change the values of the onsets\n",
    "    sorted_idxs = np.ndarray.argsort(onsets_all)\n",
    "    cumulative_add = 0\n",
    "    for onset_counter in list(range(0, len(onsets_all))):\n",
    "        # What is the onset being considered\n",
    "        onset = onsets_all[sorted_idxs[onset_counter]]\n",
    "\n",
    "        # Add time to this onset\n",
    "        onsets_all[sorted_idxs[onset_counter]] = onset + cumulative_add\n",
    "\n",
    "        # Insert the onsets at the right time\n",
    "        node_counter = int(onset_idxs[sorted_idxs[onset_counter], 0])\n",
    "        idx_counter = int(onset_idxs[sorted_idxs[onset_counter], 1])\n",
    "        onsets[node_counter][idx_counter] = onset + cumulative_add\n",
    "\n",
    "        cumulative_add += minimum_isi - 1\n",
    "\n",
    "# How long should you model\n",
    "last_event = max(map(lambda x: x[-1], onsets))  # Find the max of maxs\n",
    "duration = int(last_event + 10) # Add a decay buffer\n",
    "\n",
    "# Specify the dimensions of the volume to be created\n",
    "dimensions = np.array([template.shape[0], template.shape[1],\n",
    "                       template.shape[2], int(duration / tr_duration)])\n",
    "\n",
    "# Load the noise parameters in\n",
    "with open(noise_dict_name, 'r') as f:\n",
    "    noise_dict = f.read()\n",
    "\n",
    "noise_dict = eval(noise_dict)\n",
    "\n",
    "# Preset brain size\n",
    "brain_signal = np.zeros([dimensions[0], dimensions[1], dimensions[2],\n",
    "                         int(duration / tr_duration)], dtype='double')\n",
    "for node_counter in list(range(0, nodes)):\n",
    "\n",
    "    print('Node ' + str(node_counter))\n",
    "\n",
    "    # Preset value\n",
    "    volume = np.zeros(dimensions[0:3])\n",
    "\n",
    "    # Pull out the signal template\n",
    "    signal_pattern = signal_coords[node_counter, :]\n",
    "    onsets_node = onsets[node_counter]\n",
    "\n",
    "    # Create the time course for the signal to be generated\n",
    "    stimfunc = sim.generate_stimfunction(onsets=onsets_node,\n",
    "                                         event_durations=event_durations,\n",
    "                                         total_time=duration,\n",
    "                                         temporal_resolution=temporal_res,\n",
    "                                         )\n",
    "\n",
    "    # Aggregate the timecourse\n",
    "    if node_counter == 0:\n",
    "        stimfunc_all = np.zeros((len(stimfunc), vector_size))\n",
    "        for voxel_counter in list(range(0, vector_size)):\n",
    "            stimfunc_all[:, voxel_counter] = np.asarray(\n",
    "                stimfunc).transpose() * signal_pattern[voxel_counter]\n",
    "    else:\n",
    "\n",
    "        # Add these elements together\n",
    "        temp = np.zeros((len(stimfunc), vector_size))\n",
    "        for voxel_counter in list(range(0, vector_size)):\n",
    "            temp[:, voxel_counter] = np.asarray(stimfunc).transpose() * signal_pattern[voxel_counter]\n",
    "\n",
    "        stimfunc_all += temp\n",
    "\n",
    "# After you have gone through all the nodes, convolve the HRF and\n",
    "# stimulation for each voxel\n",
    "print('Convolving HRF')\n",
    "signal_func = sim.convolve_hrf(stimfunction=stimfunc_all,\n",
    "                               tr_duration=tr_duration,\n",
    "                               temporal_resolution=temporal_res,\n",
    "                               )\n",
    "if save_signal_func == 1 and resample == 0 and run_counter == 1:\n",
    "    plt.plot(stimfunc_all[::int(temporal_res * tr_duration), 0])\n",
    "    plt.plot(signal_func[:,0])\n",
    "    plt.xlim((0, 200))\n",
    "    plt.ylim((-1, 5))\n",
    "    plt.savefig(signal_func_save)\n",
    "\n",
    "# Convert the stim func into a binary vector of dim 1\n",
    "stimfunc_binary = np.mean(np.abs(stimfunc_all)>0, 1) > 0\n",
    "stimfunc_binary = stimfunc_binary[::int(tr_duration * temporal_res)]\n",
    "\n",
    "# Bound, can happen if the duration is not rounded to a TR\n",
    "stimfunc_binary = stimfunc_binary[0:signal_func.shape[0]]\n",
    "\n",
    "# Create the noise volumes (using the default parameters)\n",
    "noise = sim.generate_noise(dimensions=dimensions[0:3],\n",
    "                           stimfunction_tr=stimfunc_binary,\n",
    "                           tr_duration=tr_duration,\n",
    "                           template=template,\n",
    "                           mask=mask,\n",
    "                           noise_dict=noise_dict,\n",
    "                           )\n",
    "\n",
    "# Change the type of noise\n",
    "noise = noise.astype('double')\n",
    "\n",
    "# Create a noise function (same voxels for signal function as for noise)\n",
    "noise_function = noise[idxs[:, 0], idxs[:, 1], idxs[:, 2], :].T\n",
    "\n",
    "# Compute the signal magnitude for the data\n",
    "signal_func_scaled = sim.compute_signal_change(signal_function=signal_func,\n",
    "                                               noise_function=noise_function,\n",
    "                                               noise_dict=noise_dict,\n",
    "                                               magnitude=[\n",
    "                                                   signal_magnitude],\n",
    "                                               method='PSC',\n",
    "                                               )\n",
    "\n",
    "# Multiply the voxels with signal by the HRF function\n",
    "brain_signal = sim.apply_signal(signal_function=signal_func_scaled,\n",
    "                                volume_signal=signal_mask,\n",
    "                                )\n",
    "\n",
    "# Convert any nans to 0s\n",
    "brain_signal[np.isnan(brain_signal)] = 0\n",
    "\n",
    "# Combine the signal and the noise (as long as the signal magnitude is above 0)\n",
    "if signal_magnitude >= 0:\n",
    "\n",
    "    # Combine the signal and the noise\n",
    "    brain = brain_signal + noise\n",
    "\n",
    "else:\n",
    "    # Don't make signal just add it here\n",
    "    brain = brain_signal\n",
    "\n",
    "# Do you want to perform smoothing\n",
    "if fwhm > 0:\n",
    "\n",
    "    # Convert to nifti\n",
    "    brain_nifti = nibabel.Nifti1Image(brain, nii.affine)\n",
    "\n",
    "    # Run the smoothing step\n",
    "\n",
    "    sm_nii = smooth_img(brain_nifti, fwhm)\n",
    "\n",
    "    # Pull out the data again and continue on\n",
    "    brain = sm_nii.get_data()\n",
    "\n",
    "# Z score the data\n",
    "brain_Z = zscore(brain, 3)\n",
    "\n",
    "# Mask brain\n",
    "brain_signal = brain_signal * mask.reshape((mask.shape[0], mask.shape[1], mask.shape[\n",
    "    2], 1))\n",
    "\n",
    "brain_Z = brain_Z * mask.reshape((mask.shape[0], mask.shape[1], mask.shape[\n",
    "    2], 1))\n",
    "\n",
    "\n",
    "# Find the representation of each node. You can use a deconvolution approach in which I create a design matrix, convolve it with an HRF and then regress each voxel against this matrix, storing the beta values. Alternatively you can just average the TRs N seconds after event onset\n",
    "if deconvolution == 1:\n",
    "\n",
    "    # Preset\n",
    "    design_mat = np.zeros((brain.shape[3], nodes))\n",
    "\n",
    "    for node in list(range(0, nodes)):\n",
    "\n",
    "        # Create a stim function for just this node\n",
    "        stimfunc = sim.generate_stimfunction(onsets=onsets[node],\n",
    "                                 event_durations=event_durations,\n",
    "                                 total_time=duration,\n",
    "                                 temporal_resolution=temporal_res,\n",
    "                                 )\n",
    "\n",
    "        # Convolve it with the HRF\n",
    "        signal_func = sim.convolve_hrf(stimfunction=stimfunc,\n",
    "                                       tr_duration=tr_duration,\n",
    "                                       temporal_resolution=temporal_res,\n",
    "                                       )\n",
    "\n",
    "        # Store in the design matrix\n",
    "        design_mat[:, node] = signal_func.T\n",
    "\n",
    "    # Make a voxel by time array\n",
    "    voxel_time = brain_signal.reshape(np.prod(brain_signal.shape[0:3]), brain_signal.shape[3])\n",
    "\n",
    "    # Regress each voxel in the brain with the design matrix\n",
    "    voxels_reg, _, _ = regress_voxels(voxel_time,\n",
    "                                      design_mat,\n",
    "                                     )\n",
    "\n",
    "    # Unravel the data in voxels_reg\n",
    "    signal_brain = voxels_reg.reshape(brain.shape[0], \n",
    "                                        brain.shape[1],\n",
    "                                        brain.shape[2],\n",
    "                                        nodes)\n",
    "\n",
    "\n",
    "    # Make a voxel by time array\n",
    "    voxel_time = brain_Z.reshape(np.prod(brain.shape[0:3]), brain.shape[3])\n",
    "\n",
    "    # Regress each voxel in the brain with the design matrix\n",
    "    voxels_reg, _, _ = regress_voxels(voxel_time,\n",
    "                                      design_mat,\n",
    "                                     )\n",
    "\n",
    "    # Unravel the data in voxels_reg\n",
    "    node_brain = voxels_reg.reshape(brain.shape[0], \n",
    "                                        brain.shape[1],\n",
    "                                        brain.shape[2],\n",
    "                                        nodes)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all of the relevant data from this simulated run\n",
    "\n",
    "vox = 101 # Take the specified voxel index\n",
    "\n",
    "# Plot the simulated response to each event for a given signal voxel\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.bar(np.arange(num_nodes), signal_coords_all_pos[:, vox], color='k')\n",
    "plt.title('Example voxel coordinate for each event type')\n",
    "plt.xticks(np.arange(0, num_nodes, 2) + 1, np.arange(0, num_nodes, 2) + 2)\n",
    "plt.ylabel('Weight')\n",
    "plt.xlabel('Event')\n",
    "plt.savefig('../plots/methods_voxel_weights.eps')\n",
    "\n",
    "# Plot the simulated response of each voxel before any HRF convolution\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(stimfunc_all[:, vox], color='k')\n",
    "plt.xticks(np.arange(0, stimfunc_all.shape[0], temporal_res * 200), np.arange(0, stimfunc_all.shape[0] / temporal_res, 200) / 2)\n",
    "plt.title('Hypothesized voxel response to each experimental event, before convolution')\n",
    "plt.xlabel('TRs')\n",
    "plt.ylabel('Weight')\n",
    "plt.savefig('../plots/methods_stimfunc.eps')\n",
    "\n",
    "# Plot the simulated response of each voxel after convolution and scaling to the PSC\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(signal_func_scaled[:, vox])\n",
    "plt.title('Hypothesized voxel response (after convolution and scaling to 1% PSC)')\n",
    "plt.ylabel('PSC')\n",
    "plt.xlabel('TRs')\n",
    "plt.savefig('../plots/methods_convolved.eps')\n",
    "\n",
    "# Plot the simulated noise\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.title('Noise timecourse for voxel')\n",
    "plt.plot(noise[signal_mask == 1][vox, :], color='y')\n",
    "plt.xlabel('TRs')\n",
    "plt.ylabel('MR value')\n",
    "plt.savefig('../plots/methods_noise.eps')\n",
    "\n",
    "# Plot the weights for each event without noise\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.title('Deconvolved weights for signal without noise')\n",
    "plt.bar(np.arange(num_nodes), signal_brain[signal_mask == 1][vox, :], color='k')\n",
    "plt.xticks(np.arange(0, num_nodes, 2) + 1, np.arange(0, num_nodes, 2) + 2)\n",
    "plt.ylabel('Beta weights')\n",
    "plt.xlabel('Event')\n",
    "plt.savefig('../plots/methods_signal_weights.eps')\n",
    "\n",
    "# Plot the weights for each event with noise\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.title('Deconvolved weights for signal plus noise (after z-scoring)')\n",
    "plt.bar(np.arange(num_nodes), node_brain[signal_mask == 1][vox, :], color='g')\n",
    "plt.xticks(np.arange(0, num_nodes, 2) + 1, np.arange(0, num_nodes, 2) + 2)\n",
    "plt.ylabel('Beta weights')\n",
    "plt.xlabel('Event')\n",
    "plt.savefig('../plots/methods_brain_weights.eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
